{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!conda install -y -c rdkit rdkit;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/samoturk/mol2vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\n\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nimport rdkit.Chem.Fragments as f\nimport rdkit.Chem.rdMolDescriptors as d\nfrom rdkit.Chem import Lipinski\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import KBinsDiscretizer\n\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom gensim.models import word2vec\nfrom mol2vec.features import mol2alt_sentence, mol2sentence, MolSentence, DfVec, sentences2vec\nfrom gensim.models import word2vec\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.metrics import roc_curve,auc\n# from sklearn.model_selection import StratifiedKFold\nfrom scipy import interp\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(path):\n    return pd.read_csv(path, index_col=[\"INDEX\"])\n\n\ndef get_smile(dataframe):\n    data = dataframe.copy()\n    data[\"rd_form\"] = data.SMILES.apply(lambda x: Chem.MolFromSmiles(x))\n    data.drop([\"SMILES\"], inplace=True, axis=1)\n    return data\n\n\ndef get_feature(dataframe):\n    data = dataframe.copy()\n    \n#     mol_model = word2vec.Word2Vec.load('../input/assignment4datas/model_300dim.pkl')\n\n    data[\"num_atom\"] = data[\"rd_form\"].apply(lambda x:x.GetNumAtoms())\n    data[\"mol_dr\"] = data[\"rd_form\"].apply(lambda x: d.CalcExactMolWt(x))\n    data[\"halogen\"] = data[\"rd_form\"].apply(lambda x: f.fr_halogen(x))\n    data[\"nhoh_count\"] = data[\"rd_form\"].apply(lambda x: Lipinski.NHOHCount(x))\n    data[\"hacceptor_count\"] = data[\"rd_form\"].apply(lambda x: Lipinski.NumHAcceptors(x))\n    data[\"hdonor_count\"] = data[\"rd_form\"].apply(lambda x: Lipinski.NumHDonors(x))\n    data[\"hetero_count\"] = data[\"rd_form\"].apply(lambda x: Lipinski.NumHeteroatoms(x))\n    data[\"rotatable_bond_count\"] = data[\"rd_form\"].apply(lambda x: Lipinski.NumRotatableBonds(x))\n    data[\"aliphatic_ring_count\"] = data[\"rd_form\"].apply(lambda x: Lipinski.NumAliphaticRings(x))\n    data[\"aromatic_ring_count\"] = data[\"rd_form\"].apply(lambda x: Lipinski.NumAromaticRings(x))\n    \n\n    # fingerprint\n    fcfp_list = []    \n    for fcpc in range(124):\n        data[\"fcpc\" + str(fcpc)] = 0\n        fcfp_list.append(\"fcpc\" + str(fcpc))\n    fcpc_x = data[\"rd_form\"].apply(lambda x: np.array(AllChem.GetMorganFingerprintAsBitVect(x,2,nBits=124, useFeatures=True)))\n    fcpc_vector_lists = list(itertools.chain(*fcpc_x))\n    data.loc[:, fcfp_list] = np.array(fcpc_vector_lists).reshape(len(data),124)\n    \n    ecfp_list = []\n    for ecpc in range(124):\n        data[\"ecpc\" + str(ecpc)] = 0\n        ecfp_list.append(\"ecpc\" + str(ecpc))\n    ecpc_x = data[\"rd_form\"].apply(lambda x: np.array(AllChem.GetMorganFingerprintAsBitVect(x,2,nBits=124)))\n    ecpc_vector_lists = list(itertools.chain(*ecpc_x))\n    data.loc[:, ecfp_list] = np.array(ecpc_vector_lists).reshape(len(data),124)\n    \n#     # mol2vec\n#     m2v_list = []\n#     data['sentence'] = data['rd_form'].apply(lambda x: mol2alt_sentence(x, radius=2))  \n#     for m2v_idx in range(300):\n#         data[\"m2v\" + str(m2v_idx)] = 0\n#         m2v_list.append(\"m2v\" + str(m2v_idx))\n#     m2v = [DfVec(x) for x in sentences2vec(data['sentence'], mol_model, unseen='UNK')]\n#     m2v = np.array([x.vec for x in m2v])\n#     data.loc[:,m2v_list] = m2v.reshape(len(data),300)\n#     data.drop([\"sentence\"], inplace=True, axis=1)\n    \n    return data\n\n\ndef cal_auc(prob, labels):\n    f = list(zip(prob, labels))\n    rank = [values2 for values1, values2 in sorted(f, key=lambda x: x[0])]\n    rankList = [i + 1 for i in range(len(rank)) if rank[i] == 1]\n    posNum = 0\n    negNum = 0\n    for i in range(len(labels)):\n        if (labels[i] == 1):\n            posNum += 1\n        else:\n            negNum += 1\n    auc = (sum(rankList) - (posNum * (posNum + 1)) / 2) / (posNum * negNum)\n    return auc\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def binary_bayes(x_train, x_val,  y_train, y_val):\n    model = BernoulliNB()\n    model.fit(x_train, y_train)\n    prediction = model.predict_proba(x_val)\n    auc = cal_auc(prediction[:, 1], np.array(y_val))\n    return model, auc, prediction\n\n\ndef guassian_bayes(x_train, x_val,  y_train, y_val):\n    model = GaussianNB()\n    model.fit(x_train, y_train)\n    prediction = model.predict_proba(x_val)\n    auc = cal_auc(prediction[:, 1], np.array(y_val))\n    return model, auc, prediction\n\n\ndef multi_bayes(x_train, x_val,  y_train, y_val):\n    model = MultinomialNB()\n    model.fit(x_train, y_train)\n    prediction = model.predict_proba(x_val)\n    auc = cal_auc(prediction[:, 1], np.array(y_val))\n    return model, auc, prediction\n\n\ndef decision_tree(x_train, x_val,  y_train, y_val):\n    model = DecisionTreeClassifier()\n    model.fit(x_train, y_train)\n    prediction = model.predict_proba(x_val)\n    auc = cal_auc(prediction[:, 1], np.array(y_val))\n    return model, auc, prediction\n\n\ndef mlp(x_train, x_val,  y_train, y_val):\n    model = MLPClassifier()\n    model.fit(x_train, y_train)\n    prediction = model.predict_proba(x_val)\n    auc = cal_auc(prediction[:, 1], np.array(y_val))\n    return model, auc, prediction\n\n\n# def random_forest(x_train, x_val,  y_train, y_val):\n#     model = RandomForestClassifier()\n#     model.fit(x_train, y_train)\n#     prediction = model.predict_proba(x_val)\n#     auc = cal_auc(prediction[:, 1], np.array(y_val))\n#     return model, auc, prediction\ndef random_forest(x_train, x_val,  y_train, y_val, params=None):\n    if params == None:\n        model = RandomForestClassifier()\n    else:\n        model = RandomForestClassifier(**params)\n    model.fit(x_train, y_train)\n    prediction = model.predict_proba(x_val)\n    auc = cal_auc(prediction[:, 1], np.array(y_val))\n    return model, auc, prediction\n\ndef light_boost(x_train, x_val,  y_train, y_val):\n    model = lgb.LGBMClassifier()\n    model.fit(x_train, y_train)\n    prediction = model.predict_proba(x_val)\n    auc = cal_auc(prediction[:, 1], np.array(y_val))\n    return model, auc, prediction\n\n\ndef extreme_boost(x_train, x_val,  y_train, y_val):\n    model = xgb.XGBClassifier()\n    model.fit(x_train, y_train)\n    prediction = model.predict_proba(x_val)\n    auc = cal_auc(prediction[:, 1], np.array(y_val))\n    return model, auc, prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data different features \ntrain_atom = pd.read_csv('/kaggle/input/smile-feature/atom.csv', index_col=['INDEX'])\ntrain_m2v = pd.read_csv('/kaggle/input/smile-feature/m2v.csv', index_col=['INDEX'])\ntrain_fcfp = pd.read_csv('/kaggle/input/smile-feature/fcpc.csv', index_col=['INDEX'])\ntrain_ecfp = pd.read_csv('/kaggle/input/smile-feature/ecpf.csv', index_col=['INDEX'])\ny = train_atom['ACTIVE']\ndrop_list = ['ACTIVE', 'rd_form', 'COO', 'OH', 'Nhpyrrole', 'SH', 'ketones'] # this features' means equal 0\ntrain_atom = train_atom.drop(drop_list, axis=1)\ntrain_m2v = train_m2v.drop(['rd_form', 'ACTIVE'], axis=1)\ntrain_fcfp = train_fcfp.drop(['rd_form', 'ACTIVE'], axis=1)\ntrain_ecfp = train_ecfp.drop(['rd_form', 'ACTIVE'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_atom_fcfp = pd.concat([train_atom, train_fcfp], axis=1)\ntrain_atom_ecfp = pd.concat([train_atom, train_ecfp], axis=1)\ntrain_atom_m2v = pd.concat([train_atom, train_m2v], axis=1)\n# train_fcfp_ecfp = pd.concat([train_fcfp, train_ecfp], axis=1)\ntrain_atom_fcfp_ecfp = pd.concat([train_atom, train_fcfp, train_ecfp], axis=1)\ntrain_all = pd.concat([train_atom, train_fcfp, train_ecfp, train_m2v], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split and normalization\n\ndef split(data, y):\n    x_train, x_val, y_train, y_val = train_test_split(data, y, test_size=0.2, random_state=1)\n    return x_train, x_val, y_train, y_val\n\n\ndef trans(x_train, x_val, label_list):\n    train = x_train.loc[:, label_list]\n    val = x_val.loc[:, label_list]\n    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n    scaler = MinMaxScaler()\n    kbd = KBinsDiscretizer(n_bins=10, encode=\"ordinal\")\n    train = imp_mean.fit_transform(train)\n    train = scaler.fit_transform(train)\n    train = kbd.fit_transform(train)\n    val = imp_mean.transform(val)\n    val = scaler.transform(val)\n    val = kbd.transform(val)\n    x_train.loc[:, label_list] = np.array(train)\n    x_val.loc[:, label_list] = np.array(val)\n    return x_train, x_val\n\n# split and normalization\n\n# norm features\nnorm_features = list(train_atom.columns)\n# basic models results, without tuning \ndef check(train, y, norm=False, norm_list=norm_features):\n    x_train, x_val, y_train, y_val = split(train, y)\n    if norm == True:\n        x_train, x_val = trans(x_train, x_val, norm_list)\n    lb, lb_auc, lb_prediction = light_boost(x_train,x_val,y_train, y_val)\n    eb, eb_auc, en_prediction = extreme_boost(x_train,x_val,y_train, y_val)\n    rf, rf_auc, rf_prediction = random_forest(x_train,x_val,y_train, y_val)\n    baseline_r = [rf_auc, lb_auc, eb_auc]\n    print(\"rf auc without tuning: \" + str(rf_auc))\n    print(\"lgbm auc without tuning: \" + str(lb_auc))\n    print(\"xgboost auc without tuning: \" + str(eb_auc))\n    return baseline_r","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# baseline\nbaseline_rows = [\"basic\", \"fcfp\", \"ecfp\", \"m2v\", \"basic+fcfp\", \"basic+ecfp\",\n                \"basic+m2v\", \"basic+fcfp+ecfp\", \"all\"]\nbaseline_columns = [\"rf\", \"lgbm\", \"xgbm\"]\nr1 = check(train_atom, y, norm=True)\nr2 = check(train_fcfp, y)\nr3 = check(train_ecfp, y)\nr4 = check(train_m2v, y, norm_list = list(train_m2v.columns))\nr5 = check(train_atom_fcfp, y, norm=True)\nr6 = check(train_atom_ecfp, y, norm=True)\nr7 = check(train_atom_m2v, y, norm=True, norm_list = list(train_atom.columns)+\n      list(train_m2v.columns))\nr8 = check(train_atom_fcfp_ecfp, y, norm=True)\nr9 = check(train_all, y, norm=True, norm_list = list(train_atom.columns)+\n      list(train_m2v.columns))\nbaseline_r = [r1, r2, r3, r4, r5, r6, r7, r8, r9]\nr_df = pd.DataFrame(baseline_r, columns=baseline_columns, index=baseline_rows)\nr_df.to_csv('baseline_results.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hyper_tuned_rf(x_train, x_val,  y_train, y_val, model, kf, params):\n    grid_search = GridSearchCV(model, param_grid=params, cv=kf, scoring='roc_auc')\n    grid_search.fit(x_train, y_train)\n    best_params = grid_search.best_params_\n    update_model = RandomForestClassifier(**best_params)\n    update_model.fit(x_train, y_train)\n    update_prediction = update_model.predict_proba(x_val)\n    update_auc = cal_auc(update_prediction[:, 1], np.array(y_val))\n    \n    return update_model, update_prediction, update_auc, best_params, grid_search","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the curve and save the results\n\ndef rslt_plot(x_train, y_train, params, round_):\n    tprs=[]\n    aucs=[]\n    mean_fpr=np.linspace(0,1,100)\n    i = 0\n    kf = KFold(n_splits=5, random_state=42, shuffle=False)\n    fig_name = f'rf_result_fig{round_}'\n    fig = plt.figure()\n    for train_index,test_index in kf.split(x_train):\n        # divide dataset into validation\n        X_train,X_test = x_train.iloc[train_index,:],x_train.iloc[test_index,:]\n        Y_train,Y_test = y_train.iloc[train_index],y_train.iloc[test_index]\n        rf = RandomForestClassifier(**params)\n        rf.fit(X_train, Y_train)\n        Y_pred = rf.predict_proba(X_test)\n#         print(np.array(Y_pred[:,1]), Y_test)\n        fpr,tpr,thresholds=roc_curve(np.array(Y_test),Y_pred[:,1])\n\n        tprs.append(interp(mean_fpr,fpr,tpr))\n        tprs[-1][0]=0.0\n        # calculate auc\n        roc_auc=auc(fpr,tpr)\n        aucs.append(roc_auc)\n        # need plt.plot(fpr,tpr)\n        plt.plot(fpr,tpr,lw=1,alpha=0.3,label='ROC fold %d(area=%0.2f)'% (i,roc_auc))\n        i +=1    \n    plt.plot([0,1],[0,1],linestyle='--',lw=2,color='tomato',label='Active',alpha=.8)\n    mean_tpr=np.mean(tprs,axis=0)\n    mean_tpr[-1]=1.0\n    # the mean auc\n    mean_auc=auc(mean_fpr,mean_tpr)\n    std_auc=np.std(tprs,axis=0)\n    plt.plot(mean_fpr,mean_tpr,color='teal',label=r'Mean ROC (area=%0.2f)'%mean_auc,lw=2,alpha=.8)\n    std_tpr=np.std(tprs,axis=0)\n    tprs_upper=np.minimum(mean_tpr+std_tpr,1)\n    tprs_lower=np.maximum(mean_tpr-std_tpr,0)\n    plt.fill_between(mean_tpr,tprs_lower,tprs_upper,color='cornflowerblue',alpha=.2)\n    plt.xlim([-0.05,1.05])\n    plt.ylim([-0.05,1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC')\n    plt.legend(loc='lower right')\n    fig.savefig(f'{fig_name}.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_params = {\n    'n_estimators': [225, 300, 400],\n    'class_weight': ['balanced_subsample'],\n    'oob_score': [True],\n    'criterion': ['entropy'],\n    'max_depth': [30, 40]\n}\nkf = KFold(n_splits=5, random_state=42, shuffle=False)\nx_train, x_val, y_train, y_val = split(train_atom_fcfp_ecfp, y)\nx_train, x_val = trans(x_train, x_val, norm_features)\nrf, auc, pred = random_forest(x_train, x_val,  y_train, y_val)\nupdate_rf, update_rf_prediction, update_rf_auc, best_params_rf, grid_search_rf = hyper_tuned_rf(x_train, x_val, y_train, y_val,\n                                                                               model=rf, kf=kf, params=rf_params)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"rf AUC without tuning:\" + str(auc))\nprint(\"best params of RF: \" + str(best_params_rf))\nprint(\"rf AUC with best params:\" + str(update_rf_auc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_results(results, model_name):\n    df = pd.DataFrame.from_dict(grid_search_rf.cv_results_)\n    path = f'output{model_name}.csv'\n    df.to_csv(path)\n    print(\"saved \" + model_name + ' as ' + path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_results(grid_search_rf, \"rf_tuning\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_rf.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_final_params = {\n    'n_estimators': 400,\n    'class_weight': 'balanced_subsample',\n    'oob_score': True,\n    'criterion': 'entropy',\n    'max_depth': 30\n}\nnorm_list = list(train_atom.columns)\n# x_train, x_val, y_train, y_val = split(train_atom[:1000], y[:1000])\nx_train, x_val, y_train, y_val = split(train_atom_fcfp_ecfp, y)\nx_train, x_val = trans(x_train, x_val, norm_list)\n\n# plot\n# rslt_plot(x_train, y_train, rf_final_params, \"400+30\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# output the results on test dataset\ntrain_data = train_atom_fcfp_ecfp\ntest_data = pd.read_csv('../input/assignment4datas/test_smiles.csv', index_col=['INDEX'])\n\n# add features\ntest_data = get_feature(get_smile(test_data))\n# store\ntest_data.to_csv('test_features.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# normalization and remove some columns\ndrop_list = ['rd_form'] # this features' means equal 0\ntest_data = test_data.drop(drop_list, axis=1)\n# norm_list = list(train_atom.columns)\nx_train, x_val, y_train, y_val = split(train_data, y)\nx_train_ = x_train\nx_train, x_val = trans(x_train, x_val, norm_features)\nx_train_, test_data = trans(x_train_, test_data, norm_features) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_rf, final_auc, final_val_prediction = random_forest(x_train, x_val,  y_train, y_val, rf_final_params)\nprint(\"final auc on val data: \" + str(final_auc))\nfinal_results = final_rf.predict_proba(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rr = pd.DataFrame(final_results, columns=['active=0.0', 'active=1.0'], index=list(test_data.index))\nrr.to_csv('final_results.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt = 0\nfor i in range(len(rr)):\n    if rr.loc[i+121375, 'active=0.0']>0.5:\n        cnt += 1\n\ncnt","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}